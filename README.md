# Custom PPO vs. DQN and A2C on CartPole v1

This project implements and compares the performance of PPO, DQN, and A2C algorithms on the CartPole-v1 environment using Python.

## Setup Instructions

1. **Clone the Repository**:
```bash
   git clone https://github.com/namrapatel/PPO 
   cd PPO
```

2. **Install Dependencies**: Ensure you have Python 3.8+ installed. Then, install the required packages:

```bash
pip install -r requirements.txt
```

3. Go